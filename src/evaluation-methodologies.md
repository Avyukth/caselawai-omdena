
# Evaluation Methodologies

We used the RAGAS framework to evaluate our RAG systems, focusing on the following metrics:

- Faithfulness: Measures how well the generated answer aligns with the provided context
- Answer Relevance: Assesses how well the answer addresses the original question
- Context Relevance: Evaluates the relevance of retrieved context chunks
- Context Recall: Measures how well the retrieved context aligns with the ground truth answer

## 7. Evaluation Results

[This section needs to be filled with numerical results after testing]

## 8. Rationale for Choosing Hybrid RAG

We ultimately chose the Hybrid RAG approach for the following reasons:

1. Comprehensive Information Capture: Captures both structured relationships and semantic nuances
2. Enhanced Entity Representation: Maintains clear representation of legal entities and relationships
3. Improved Semantic Understanding: Ensures deep understanding of complex legal language
4. Flexible Querying: Allows for both structured and open-ended legal queries
5. Improved Accuracy: Leverages both structured and unstructured information for comprehensive responses
